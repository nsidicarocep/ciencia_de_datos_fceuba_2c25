<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Regresi√≥n Lineal</title>
    <meta charset="utf-8" />
    <meta name="author" content="Prof.¬†Nicol√°s Sidicaro" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Regresi√≥n Lineal
]
.subtitle[
## Fundamentos para Econom√≠a y Ciencia de Datos
]
.author[
### Prof.¬†Nicol√°s Sidicaro
]
.date[
### Septiembre 2025
]

---




class: inverse, center, middle

# Objetivos de la clase

### ‚Ä¢ Comprender qu√© es la regresi√≥n lineal y cu√°ndo usarla
### ‚Ä¢ Distinguir entre enfoque econom√©trico y de machine learning
### ‚Ä¢ Identificar los supuestos fundamentales y su importancia
### ‚Ä¢ Interpretar coeficientes en diferentes especificaciones
### ‚Ä¢ Diagnosticar problemas comunes y sus soluciones

---

# ¬øQu√© es la regresi√≥n lineal?

## Definici√≥n intuitiva

**Regresi√≥n lineal**: m√©todo para modelar la relaci√≥n entre:
- Una **variable dependiente** (Y): lo que queremos explicar/predecir
- Una o m√°s **variables independientes** (X): factores que influyen

--

## La idea fundamental

Encontrar la **mejor l√≠nea** que describe c√≥mo Y cambia cuando cambian las X

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Educaci√≥n + Œ≤‚ÇÇ √ó Experiencia + Œµ
```

Donde:
- **Œ≤‚ÇÄ**: Intercepto (punto de partida)
- **Œ≤‚ÇÅ, Œ≤‚ÇÇ**: Pendientes (efectos de cada X sobre Y)
- **Œµ**: Error (lo que el modelo no explica)

---

# Visualizaci√≥n: la intuici√≥n geom√©trica

.pull-left[
## Regresi√≥n simple (una X)

```
Salario ($)
    ‚îÇ
120k‚îÇ              √ó
    ‚îÇ        √ó √ó
 80k‚îÇ    √ó     
    ‚îÇ√ó  
 40k‚îÇ  
    ‚îÇ_______________
       8  12  16  20
    A√±os educaci√≥n
```

**Objetivo**: Minimizar suma de errores¬≤
]

.pull-right[
## Regresi√≥n m√∫ltiple

Con m√°s variables:
- Ya no es una l√≠nea
- Es un **hiperplano**
- En m√∫ltiples dimensiones

```r
Salario = f(Educaci√≥n, 
            Experiencia, 
            G√©nero, 
            Regi√≥n, ...)
```
]

---

# ¬øPor qu√© "m√≠nimos cuadrados"?

## El principio de estimaci√≥n (MCO/OLS)

1. Para cada observaci√≥n i:
   - Valor real: y&lt;sub&gt;i&lt;/sub&gt;
   - Valor predicho: ≈∑&lt;sub&gt;i&lt;/sub&gt; = Œ≤‚ÇÄ + Œ≤‚ÇÅx&lt;sub&gt;1i&lt;/sub&gt; + ...
   - Error: e&lt;sub&gt;i&lt;/sub&gt; = y&lt;sub&gt;i&lt;/sub&gt; - ≈∑&lt;sub&gt;i&lt;/sub&gt;

--

2. Queremos **minimizar**: Œ£(e&lt;sub&gt;i&lt;/sub&gt;¬≤)

--

3. ¬øPor qu√© cuadrados?
   - ‚úÖ Penaliza errores grandes m√°s que peque√±os
   - ‚úÖ Errores + y - no se cancelan
   - ‚úÖ Soluci√≥n matem√°tica elegante
   - ‚ö†Ô∏è Sensible a outliers

---

# Casos de uso en econom√≠a

## 1. An√°lisis causal (Econometr√≠a cl√°sica)

**¬øCu√°l es el efecto de X sobre Y?**

| Pregunta | Variable Y | Variables X clave |
|----------|-----------|-------------------|
| Retorno a educaci√≥n | Salario | A√±os educaci√≥n, experiencia |
| Efecto salario m√≠nimo | Desempleo | Salario m√≠nimo, PIB |
| Discriminaci√≥n salarial | Salario | G√©nero, educaci√≥n |

**Enfoque**: Identificar efecto **causal**, controlar confusores

---

# Casos de uso en econom√≠a

## 2. Predicci√≥n (Machine Learning)

**¬øCu√°l ser√° el valor futuro de Y dados X?**

| Aplicaci√≥n | Variable Y | Features |
|------------|-----------|----------|
| Precio viviendas | Precio | m¬≤, ubicaci√≥n, antig√ºedad |
| Demanda producto | Ventas | Precio, publicidad |
| Credit scoring | Default | Ingresos, deuda, historial |

**Enfoque**: Maximizar poder predictivo, validaci√≥n out-of-sample

---

## 3. An√°lisis descriptivo

**¬øQu√© factores se asocian con Y?**

- Identificar correlaciones importantes
- Explorar relaciones multivariadas
- Generar hip√≥tesis para investigaci√≥n
- Benchmarking entre unidades

---

class: inverse, center, middle

# Supuestos de Gauss-Markov

---

# ¬øPor qu√© importan los supuestos?

Los supuestos de Gauss-Markov **garantizan** que MCO sea el **BLUE**:

.pull-left[
- **B**est (mejor)
- **L**inear (lineal)
- **U**nbiased (insesgado)
- **E**stimator (estimador)
]

.pull-right[
**Es decir**: MCO es el estimador lineal de **menor varianza** entre todos los insesgados
]

---

# Los 5 supuestos fundamentales

## Supuesto 1: Linealidad

El modelo es **lineal en los par√°metros**:

```
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ... + Œ≤‚ÇñX‚Çñ + Œµ
```

**Nota importante**: Lineal en par√°metros ‚â† lineal en variables

- ‚úÖ Esto S√ç es lineal: Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ (lineal en Œ≤)
- ‚ùå Esto NO: Y = Œ≤‚ÇÄ + X^Œ≤‚ÇÅ (no lineal en Œ≤)

--

**Consecuencia si se viola**: MCO no es apropiado

---

# Supuesto 2: Exogeneidad estricta

Los errores no est√°n correlacionados con las X:

```
E(Œµ | X‚ÇÅ, X‚ÇÇ, ..., X‚Çñ) = 0
```

Equivalente a: **Cov(X&lt;sub&gt;j&lt;/sub&gt;, Œµ) = 0** para todo j

--

**En lenguaje simple**: Las X son **ex√≥genas**, no est√°n correlacionadas con factores omitidos en el error

--

**Consecuencia si se viola**: 
- ‚ùå **Sesgo en los coeficientes** (estimaci√≥n incorrecta)
- Problema m√°s grave: afecta inferencia causal

---

# Causas comunes de violaci√≥n

## Variables omitidas

```r
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Educaci√≥n + Œµ
```

Omitimos **Habilidad**, que:
- Afecta Salario (est√° en Œµ)
- Correlacionada con Educaci√≥n

‚Üí Œ≤ÃÇ‚ÇÅ est√° **sesgado hacia arriba**

--

## Simultaneidad (causalidad reversa)

```r
Cantidad = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Precio + Œµ
```

- Precio afecta Cantidad
- Pero Cantidad tambi√©n afecta Precio (equilibrio)

---

# Supuesto 3: No multicolinealidad perfecta

Ninguna X puede ser **combinaci√≥n lineal perfecta** de otras X

**Ejemplos**:

.pull-left[
‚úÖ **OK**:  
```r
Y ~ Educaci√≥n + Experiencia
```
]

.pull-right[
‚ùå **MAL**: 
```r
Y ~ Edad + Experiencia + 
    (Edad - Experiencia)
```
]

--

**Consecuencia**: No se pueden estimar los coeficientes

**Multicolinealidad imperfecta** (m√°s com√∫n):
- X correlacionadas pero no perfectamente
- Coeficientes siguen insesgados
- Pero **alta varianza** (menos precisi√≥n)

---

# Supuesto 4: Homoscedasticidad

La varianza del error es **constante** para todos los valores de X:

```
Var(Œµ | X) = œÉ¬≤
```

.pull-left[
**Homoced√°stico**:
```
Residuos
  2‚îÇ √ó √ó √ó √ó
  0‚îÇ√ó √ó √ó √ó √ó
 -2‚îÇ √ó √ó √ó
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     Fitted
```
]

.pull-right[
**Heteroced√°stico**:
```
Residuos
  2‚îÇ      √ó √ó
  0‚îÇ  √ó √ó √ó
 -2‚îÇ√ó
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     Fitted
  (fan shape)
```
]

--

**Consecuencia si se viola**:
- ‚úÖ Coeficientes insesgados
- ‚ùå Errores est√°ndar incorrectos ‚Üí inferencia err√≥nea

---

# Supuesto 5: No autocorrelaci√≥n

Los errores son **independientes** entre observaciones:

```
Cov(Œµ·µ¢, Œµ‚±º) = 0  para todo i ‚â† j
```

**Cu√°ndo es relevante**:
- Series temporales
- Panel data  
- Datos espaciales

--

**Ejemplo**: Regresi√≥n de PIB trimestral
- Si Œµ&lt;sub&gt;t&lt;/sub&gt; &gt; 0, probablemente Œµ&lt;sub&gt;t+1&lt;/sub&gt; &gt; 0
- Errores "persisten" en el tiempo

--

**Consecuencia**:
- ‚úÖ Coeficientes insesgados
- ‚ùå Errores est√°ndar incorrectos

---

# Resumen de supuestos y consecuencias

| Supuesto | Violaci√≥n | Œ≤ÃÇ | SE(Œ≤ÃÇ) | Soluci√≥n |
|----------|-----------|-----|-------|----------|
| Linealidad | Especif. err√≥nea | ‚ùå Sesgado | ‚ùå Incorrecto | Transformaci√≥n |
| Exogeneidad | Endogeneidad | ‚ùå Sesgado | ‚ùå Incorrecto | IV, controles |
| No multicolinealidad | Multicolin. | ‚úÖ Insesgado | ‚ùå Alta var | Eliminar vars |
| Homoscedasticidad | Heterosc. | ‚úÖ Insesgado | ‚ùå Incorrecto | Errores robustos |
| No autocorrelaci√≥n | Autocorr. | ‚úÖ Insesgado | ‚ùå Incorrecto | Errores HAC |

--

**Jerarqu√≠a de gravedad**:
1. üî¥ **Endogeneidad**: Sesgo ‚Üí inferencia causal imposible
2. üü° **Hetero/autocorrelaci√≥n**: Solo inferencia incorrecta
3. üü¢ **Multicolinealidad**: Solo precisi√≥n afectada

---

class: inverse, center, middle

# Econometr√≠a vs Machine Learning

---

# El gran contraste

|  | **Econometr√≠a** | **Machine Learning** |
|--|----------------|---------------------|
| **Objetivo** | Inferencia causal | Predicci√≥n |
| **Pregunta** | ¬øEfecto de X sobre Y? | ¬øCu√°l ser√° Y dado X? |
| **Preocupaci√≥n** | Sesgo en coeficientes | Error out-of-sample |
| **Supuestos** | Expl√≠citos y cr√≠ticos | Flexibles |
| **Interpretabilidad** | Fundamental | Secundaria |
| **Complejidad** | Parsimonia | Muchas features |
| **Evaluaci√≥n** | Significancia, R¬≤ | RMSE, validaci√≥n cruzada |
| **Overfitting** | Menos preocupante | Preocupaci√≥n central |

---

# Ejemplo concreto: Determinantes salariales

.pull-left[
## Perspectiva econom√©trica

```r
modelo &lt;- lm(
  log_salario ~ educacion + 
                experiencia + 
                female,
  data = wage1
)
```

**Foco**: Œ≤_educacion  
**Interpretaci√≥n**: Retorno causal  
**Preocupaci√≥n**: ¬øEndogeneidad?
]

.pull-right[
## Perspectiva ML

```r
modelo_ml &lt;- lm(
  salario ~ educacion + 
            experiencia + 
            female + region + 
            sector + ocupacion +
            horas + ...,
  data = train_data
)
```

**Foco**: RMSE en test  
**Preocupaci√≥n**: ¬øOverfitting?
]

---

# El trade-off sesgo-varianza

.pull-left[
## Econometr√≠a

- Priorizamos **insesgadez**
- Aceptamos mayor varianza
- Modelos parsimoniosos
]

.pull-right[
## Machine Learning

- Aceptamos sesgo si reduce varianza
- Minimizar **MSE = sesgo¬≤ + varianza**
- Regularizaci√≥n introduce sesgo
]

--

```
Error de     ‚îÇ     
predicci√≥n   ‚îÇ   ‚ï± Varianza
             ‚îÇ  ‚ï±___  Error total (MSE)
             ‚îÇ      ‚ï≤___ Sesgo¬≤
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
               Simple ‚Üí Complejo
```

---

# ¬øCu√°ndo usar cada enfoque?

## Usa enfoque econom√©trico cuando:

- Quieres entender **causas**
- Necesitas estimar **efectos de pol√≠ticas**
- La interpretaci√≥n es cr√≠tica
- Tienes **teor√≠a econ√≥mica** que gu√≠a el modelo

**Ejemplos**: Evaluaci√≥n de impacto, retornos a educaci√≥n

--

## Usa enfoque ML cuando:

- Quieres **predecir** valores futuros
- No importa tanto el "por qu√©"
- Tienes **muchas variables** potenciales
- Necesitas m√°xima precisi√≥n predictiva

**Ejemplos**: Credit scoring, forecast de ventas

---

class: inverse, center, middle

# Interpretaci√≥n de Coeficientes

---

# La interpretaci√≥n fundamental

En regresi√≥n lineal simple:

```
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ
```

**Œ≤‚ÇÅ** representa: 

&gt; "El cambio en Y asociado con un aumento de **una unidad** en X, manteniendo todo lo dem√°s constante (ceteris paribus)"

--

**Œ≤‚ÇÄ** representa: 

&gt; El valor esperado de Y cuando X = 0 (a menudo sin interpretaci√≥n sustantiva)

---

# Modelos con diferentes especificaciones

## 1. Nivel-Nivel (lineal puro)

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Educaci√≥n + Œµ
```

**Interpretaci√≥n de Œ≤‚ÇÅ**: 
- "Un a√±o adicional de educaci√≥n se asocia con un incremento de **Œ≤‚ÇÅ pesos** en el salario"
- Efecto **aditivo**

**Ejemplo**: 
- Si Œ≤‚ÇÅ = 5000: cada a√±o suma $5,000

---

# 2. Log-Nivel (Y en logaritmo)

```
log(Salario) = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Educaci√≥n + Œµ
```

**Interpretaci√≥n de Œ≤‚ÇÅ**:
- "Un a√±o adicional de educaci√≥n se asocia con un incremento de **Œ≤‚ÇÅ √ó 100%** en el salario"
- Efecto **porcentual**

--

**Ejemplo**: 
- Si Œ≤‚ÇÅ = 0.10: cada a√±o aumenta salario en **10%**

**Ventajas**:
- Captura retornos porcentuales (m√°s realista)
- Reduce heteroscedasticidad
- Coeficientes comparables entre contextos

---

# 3. Nivel-Log (X en logaritmo)

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó log(PIB_per_capita) + Œµ
```

**Interpretaci√≥n de Œ≤‚ÇÅ**:
- "Un incremento del 1% en PIB per c√°pita se asocia con un incremento de **Œ≤‚ÇÅ/100 pesos** en salario"

**√ötil cuando**:
- X tiene rangos muy amplios (PIB, poblaci√≥n)
- Relaci√≥n de rendimientos decrecientes

---

# 4. Log-Log (ambas en logaritmo)

```
log(Salario) = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó log(Experiencia) + Œµ
```

**Interpretaci√≥n de Œ≤‚ÇÅ**:
- "Un incremento del 1% en experiencia se asocia con un incremento de **Œ≤‚ÇÅ%** en salario"
- Œ≤‚ÇÅ es una **elasticidad**

--

**Ejemplo**:
- Si Œ≤‚ÇÅ = 0.3: aumento del 10% en experiencia ‚Üí +3% en salario

**Ventajas**:
- Interpretaci√≥n como elasticidad (concepto econ√≥mico)
- Relaci√≥n proporcional
- Muy com√∫n en econometr√≠a

---

# Resumen de especificaciones

| Modelo | Ecuaci√≥n | Interpretaci√≥n Œ≤‚ÇÅ | Ejemplo |
|--------|----------|-------------------|---------|
| **Nivel-Nivel** | Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX | ŒîY = Œ≤‚ÇÅ cuando ŒîX = 1 | $5,000 m√°s/a√±o |
| **Log-Nivel** | log(Y) = Œ≤‚ÇÄ + Œ≤‚ÇÅX | %ŒîY = 100Œ≤‚ÇÅ cuando ŒîX = 1 | 10% m√°s/a√±o |
| **Nivel-Log** | Y = Œ≤‚ÇÄ + Œ≤‚ÇÅlog(X) | ŒîY = Œ≤‚ÇÅ/100 cuando %ŒîX = 1 | $5 por 1% m√°s PIB |
| **Log-Log** | log(Y) = Œ≤‚ÇÄ + Œ≤‚ÇÅlog(X) | %ŒîY = Œ≤‚ÇÅ cuando %ŒîX = 1 | Elasticidad 0.3 |

---

# Variables dummy (categ√≥ricas)

## Dummy simple

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Female + Œ≤‚ÇÇ √ó Educaci√≥n + Œµ
```

Donde `Female = 1` si mujer, `0` si hombre

**Interpretaci√≥n**:
- **Œ≤‚ÇÄ**: Salario esperado para hombres con educaci√≥n = 0
- **Œ≤‚ÇÅ**: Diferencia salarial entre mujeres y hombres con misma educaci√≥n
  - Si Œ≤‚ÇÅ = -5000: brecha de g√©nero de $5,000

---

# M√∫ltiples categor√≠as

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Secundaria + Œ≤‚ÇÇ √ó Universitaria + Œµ
```

Categor√≠a de referencia: **Primaria** (omitida)

**Interpretaci√≥n**:
- Œ≤‚ÇÅ: Diferencia entre secundaria y primaria
- Œ≤‚ÇÇ: Diferencia entre universitaria y primaria

--

‚ö†Ô∏è **Regla**: Si tenemos K categor√≠as, incluimos K-1 dummies

---

# Interacciones

## Interacci√≥n continua √ó dummy

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óEducaci√≥n + Œ≤‚ÇÇ√óFemale + 
          Œ≤‚ÇÉ√ó(Educaci√≥n √ó Female) + Œµ
```

**Interpretaci√≥n**:
- **Œ≤‚ÇÅ**: Retorno a educaci√≥n para hombres
- **Œ≤‚ÇÅ + Œ≤‚ÇÉ**: Retorno a educaci√≥n para mujeres
- **Œ≤‚ÇÉ**: Diferencia en el retorno a educaci√≥n

--

**Ejemplo**:
- Si Œ≤‚ÇÅ = 6000 y Œ≤‚ÇÉ = -1000:
  - Hombres: +$6,000 por a√±o
  - Mujeres: +$5,000 por a√±o

---

# T√©rminos cuadr√°ticos

```
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Experiencia + Œ≤‚ÇÇ √ó Experiencia¬≤ + Œµ
```

**Interpretaci√≥n**:
- Relaci√≥n **no lineal**
- Efecto marginal **var√≠a** con experiencia

**Efecto marginal**:
```
‚àÇSalario/‚àÇExperiencia = Œ≤‚ÇÅ + 2Œ≤‚ÇÇ √ó Experiencia
```

--

**Ejemplo t√≠pico**: Retorno a experiencia
- Œ≤‚ÇÅ &gt; 0, Œ≤‚ÇÇ &lt; 0: Rendimientos decrecientes
- Punto de m√°ximo: Experiencia* = -Œ≤‚ÇÅ/(2Œ≤‚ÇÇ)

---

# Errores comunes en interpretaci√≥n

‚ùå **Error 1**: Causalidad autom√°tica
- Regresi√≥n muestra **asociaci√≥n**, no causalidad

‚ùå **Error 2**: Ceteris paribus olvidado
- El efecto es "manteniendo todo lo dem√°s constante"

‚ùå **Error 3**: Extrapolar fuera del rango
- V√°lido solo en rango observado

‚ùå **Error 4**: Ignorar significancia
- Reportar: coeficiente, error est√°ndar, p-value

‚ùå **Error 5**: Confundir unidades
- "Aumenta en X%" vs "aumenta en X pesos"

---

class: inverse, center, middle

# Evaluaci√≥n de Modelos

---

# ¬øC√≥mo sabemos si el modelo es "bueno"?

**Depende del objetivo**:

.pull-left[
## Econometr√≠a (causal)

1. ¬øSignos esperados?
2. ¬øSignificancia estad√≠stica?
3. ¬øSupuestos se cumplen?
4. ¬øParsimonia?
]

.pull-right[
## ML (predicci√≥n)

1. ¬øPredice bien en test?
2. ¬øError aceptable?
3. ¬øHay overfitting?
]

---

# R¬≤ y R¬≤ ajustado

## R¬≤ (coeficiente de determinaci√≥n)

```
R¬≤ = 1 - (SSR / SST)
```

**Interpretaci√≥n**:
- R¬≤ = 0.75: "El modelo explica 75% de la variabilidad de Y"
- Rango: [0, 1]

--

**Problema**: R¬≤ **siempre aumenta** al agregar variables

---

# R¬≤ ajustado (penaliza complejidad)

```
R¬≤_adj = 1 - [(1 - R¬≤) √ó (n - 1) / (n - k - 1)]
```

**Ventaja**: 
- Solo aumenta si nueva variable **realmente mejora** ajuste
- Penaliza modelos complejos
- √ötil para **comparar modelos**

--

**Uso pr√°ctico**:
- Modelo 1: R¬≤ = 0.75, k = 5
- Modelo 2: R¬≤ = 0.76, k = 15
- Si R¬≤_adj mayor en Modelo 1 ‚Üí preferir Modelo 1

---

# AIC y BIC

**AIC (Akaike)**:
```
AIC = -2 √ó log(L) + 2k
```

**BIC (Bayesian)**:
```
BIC = -2 √ó log(L) + k √ó log(n)
```

**Interpretaci√≥n**:
- **Menor es mejor**
- Balancean ajuste con complejidad
- BIC penaliza m√°s que AIC

---

# Significancia estad√≠stica

## Test t para coeficientes individuales

**Hip√≥tesis**:
- H‚ÇÄ: Œ≤‚±º = 0 (variable no tiene efecto)
- H‚ÇÅ: Œ≤‚±º ‚â† 0 (variable s√≠ tiene efecto)

**Estad√≠stico t**:
```
t = Œ≤ÃÇ‚±º / SE(Œ≤ÃÇ‚±º)
```

--

**P-value**:
- p &lt; 0.05: Variable significativa al 5%
- p &lt; 0.01: Significativa al 1%
- p &gt; 0.05: No significativa

---

# Intervalos de confianza

**IC al 95% para Œ≤‚±º**:
```
Œ≤ÃÇ‚±º ¬± 1.96 √ó SE(Œ≤ÃÇ‚±º)
```

**Interpretaci√≥n**:
- "Con 95% de confianza, el verdadero efecto est√° entre [a, b]"
- Si incluye 0: no significativo
- Si no incluye 0: significativo

--

**Ejemplo**:
```
Educaci√≥n: 
  Coeficiente: 5000
  IC 95%: [3200, 6800]
```

---

# Significancia vs Relevancia pr√°ctica

‚ö†Ô∏è **Distinci√≥n crucial**:

**Significancia estad√≠stica** ‚â† **Relevancia pr√°ctica**

--

**Ejemplo**:
```
Coeficiente: 0.50
Error est√°ndar: 0.10
p-value: &lt; 0.001
```

Estad√≠sticamente significativo, pero... ¬ø50 centavos es relevante?

--

**Con muestras grandes**: cualquier efecto puede ser significativo

**Reporte apropiado**: Magnitud + significancia + contexto

---

# M√©tricas de predicci√≥n (ML)

**RMSE** (Root Mean Squared Error):
```
RMSE = ‚àö[Œ£(y·µ¢ - ≈∑·µ¢)¬≤ / n]
```
- Misma unidad que Y
- Penaliza errores grandes

**MAE** (Mean Absolute Error):
```
MAE = Œ£|y·µ¢ - ≈∑·µ¢| / n
```
- Robusta a outliers
- Interpretaci√≥n simple

---

# Train-Test split

**Validaci√≥n out-of-sample**:

```r
# Dividir datos
train_index &lt;- sample(1:n, 0.7*n)
train &lt;- data[train_index, ]
test &lt;- data[-train_index, ]

# Entrenar
modelo &lt;- lm(y ~ x, data = train)

# Evaluar
pred &lt;- predict(modelo, test)
rmse_test &lt;- sqrt(mean((test$y - pred)^2))
```

--

**Por qu√© importa**:
- R¬≤ en train puede enga√±ar
- Test RMSE es verdadera medida
- Si RMSE_test &gt;&gt; RMSE_train: **overfitting**

---

class: inverse, center, middle

# Causalidad vs Correlaci√≥n

---

# El problema fundamental

**Pregunta causal**: ¬øEfecto de educaci√≥n universitaria sobre ingresos?

**Problema**: No observamos misma persona con y sin t√≠tulo

--

**Sesgo de selecci√≥n**:
- Personas que van a universidad son **diferentes**
- Habilidad, motivaci√≥n ‚Üí afectan tanto probabilidad de ir como ingresos

--

**Regresi√≥n simple**:
```r
lm(ingresos ~ universitario)
```

Captura **correlaci√≥n**, no causalidad

---

# El rol de las variables de control

**Idea**: Controlar por **confusores**

```r
# Modelo ingenuo (sesgado)
lm(ingresos ~ universitario)

# Modelo con controles (menos sesgado)
lm(ingresos ~ universitario + habilidad + 
              contexto_familiar + educacion_padres)
```

--

**Supuesto fuerte**: Despu√©s de controlar, no hay otros confusores

**Problema**: ¬øC√≥mo sabemos que controlamos todo?

---

# Estrategias para inferencia causal

## 1. Variables Instrumentales (IV)

Encontrar Z que:
- Afecta X (educaci√≥n)
- NO afecta Y (ingresos) directamente
- NO correlacionada con confusores

**Ejemplo**: Proximidad a universidad

---

## 2. Difference-in-Differences (DiD)

Para evaluaci√≥n de pol√≠ticas:
- Grupo tratado vs control
- Antes vs despu√©s

**Ejemplo**: Efecto de salario m√≠nimo en empleo

---

## 3. Regression Discontinuity (RD)

Cuando hay cutoff arbitrario:
- Ejemplo: Becas para puntaje &gt; 75
- Comparar alumnos cerca del cutoff

**Supuesto**: Cerca del cutoff, alumnos similares

---

# Correlaci√≥n para predicci√≥n

**En ML, causalidad no es necesaria**:

```r
modelo_churn &lt;- glm(churn ~ saldo + transacciones + 
                            edad + productos + ...,
                    family = binomial)
```

**No importa**:
- Si saldo causa churn
- Si churn causa cambio en saldo
- Si ambos son causados por tercera variable

**Importa**: ¬øPredice bien?

---

# Resumen: Tres objetivos

| Objetivo | Pregunta | M√©todo | Causalidad |
|----------|----------|--------|-----------|
| **Descripci√≥n** | ¬øQu√© se asocia con Y? | Regresi√≥n m√∫ltiple | No |
| **Predicci√≥n** | ¬øCu√°l ser√° Y? | ML + validaci√≥n | No |
| **Causalidad** | ¬øEfecto de X en Y? | Dise√±o causal | S√≠ ‚úì |

--

**Errores comunes**:
- ‚ùå Regresi√≥n para causalidad sin dise√±o
- ‚ùå Interpretar correlaci√≥n como causalidad
- ‚ùå "Controlar por todo" no garantiza causalidad

---

class: inverse, center, middle

# Vulneraci√≥n de Supuestos

---

# 1. Multicolinealidad

**Definici√≥n**: Variables independientes **altamente correlacionadas**

**Ejemplos**:
- Edad y Experiencia (corr ‚âà 0.9)
- PIB y Consumo (corr ‚âà 0.95)

--

**Detecci√≥n**:
```r
# VIF (Variance Inflation Factor)
library(car)
vif(modelo)
# VIF &gt; 10: problem√°tico
```

---

# Consecuencias de multicolinealidad

‚úÖ **No afecta**:
- Coeficientes insesgados
- Predicciones v√°lidas
- R¬≤

--

‚ùå **S√≠ afecta**:
- **Alta varianza** de coeficientes
- Dificulta significancia individual
- Signos contraintuitivos
- Coeficientes inestables

---

# Soluciones para multicolinealidad

**1. Eliminar variables redundantes**
```r
# Elegir solo una
modelo &lt;- lm(salario ~ educacion + experiencia)
```

**2. Combinar variables**
```r
indice_habilidad &lt;- (verbal + matematica) / 2
```

**3. Aumentar n** (si posible)

**4. Regularizaci√≥n** (Ridge) - enfoque ML

**5. PCA** - pierde interpretabilidad

---

# Cu√°ndo preocuparse

**No es problema si**:
- Solo importa predicci√≥n
- Variables correlacionadas no son centrales
- VIF &lt; 5

**Es problema si**:
- Quieres interpretar coeficientes individuales
- VIF &gt; 10
- Coeficientes cambian mucho
- Signos inesperados

---

# 2. Heteroscedasticidad

**Definici√≥n**: Varianza del error **no es constante**

**Ejemplos en econom√≠a**:
- Ingresos: Mayor varianza para alta educaci√≥n
- Gasto: Mayor varianza para alto ingreso
- Retornos: Mayor varianza en crisis

--

**Detecci√≥n**:
```r
# Gr√°fico
plot(modelo, which = 1)

# Test de Breusch-Pagan
library(lmtest)
bptest(modelo)
```

---

# Consecuencias

‚úÖ **No afecta**:
- Coeficientes insesgados
- Predicciones puntuales
- R¬≤

--

‚ùå **S√≠ afecta**:
- **Errores est√°ndar incorrectos**
- **Tests t y F inv√°lidos**
- **IC incorrectos**

**Gravedad**: Moderada (solo inferencia)

---

# Soluciones

## 1. Errores est√°ndar robustos ‚≠ê M√°s com√∫n

```r
library(sandwich)
library(lmtest)

coeftest(modelo, vcov = vcovHC(modelo, type = "HC1"))
```

**Interpretaci√≥n**:
- Coeficientes iguales
- SE corregidos
- Inferencia v√°lida

---

## 2. Weighted Least Squares (WLS)

```r
modelo_wls &lt;- lm(salario ~ educacion,
                 weights = 1/educacion)
```

**Ventaja**: M√°s eficiente  
**Desventaja**: Requiere conocer estructura

--

## 3. Transformaci√≥n logar√≠tmica

```r
modelo_log &lt;- lm(log(salario) ~ educacion)
```

**Por qu√© funciona**: Comprime rango de Y

---

# Recomendaci√≥n pr√°ctica

**En econometr√≠a moderna**:

‚úÖ **Siempre usar errores robustos**
- Est√°ndar en papers
- No hace da√±o si no hay hetero
- Protege contra hetero no detectada

**En ML**:
- Menos relevante (no hacemos inferencia)
- Si importa predicci√≥n, no es problema

---

# 3. Endogeneidad

**Definici√≥n**: X correlacionadas con error

```
Cov(X, Œµ) ‚â† 0
```

**Consecuencia**: **Sesgo en coeficientes**

‚ö†Ô∏è **El problema m√°s grave**

---

# Causas de endogeneidad

## 1. Variables omitidas

```r
Salario = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Educaci√≥n + Œµ
```

Omitimos **Habilidad**:
- Afecta Salario (est√° en Œµ)
- Correlacionada con Educaci√≥n

‚Üí Œ≤ÃÇ‚ÇÅ **sesgado hacia arriba**

---

## 2. Simultaneidad

**Ejemplo**: Oferta y demanda
```r
Cantidad = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Precio + Œµ
```

- Precio afecta Cantidad
- Cantidad tambi√©n afecta Precio

‚Üí Œ≤ÃÇ‚ÇÅ **sesgado**

---

## 3. Errores de medici√≥n

**Ejemplo**: Educaci√≥n reportada con error

Si usamos Educaci√≥n* en regresi√≥n:
- Œ≤ÃÇ‚ÇÅ **sesgado hacia cero** (attenuation bias)

---

# Soluciones

## 1. Variables Instrumentales (IV)

Encontrar Z que:
1. Correlacionada con X
2. NO correlacionada con Œµ

```r
library(AER)
modelo_iv &lt;- ivreg(
  salario ~ educacion | proximidad_universidad
)
```

---

## 2. Variables de control

```r
# Menos sesgado
lm(salario ~ educacion + habilidad + 
            contexto_familiar)
```

**Limitaci√≥n**: Solo si variable omitida es observable

---

## 3. Panel data con efectos fijos

```r
library(plm)
modelo_fe &lt;- plm(salario ~ educacion,
                 model = "within")
```

**Idea**: Controla caracter√≠sticas no observables constantes

---

## 4. Difference-in-Differences

```r
modelo_did &lt;- lm(outcome ~ tratado * post)
```

---

## 5. Regression Discontinuity

```r
modelo_rd &lt;- lm(outcome ~ tratado + running_var +
                          tratado:running_var)
```

---

# Cu√°ndo preocuparse

**Alta prioridad si**:
- Objetivo es causalidad
- Sospecha te√≥rica de endogeneidad
- Coeficientes inesperados

**Baja prioridad si**:
- Objetivo es solo predicci√≥n
- Variables claramente ex√≥genas

---

# Resumen de problemas

| Problema | Consecuencia | Soluci√≥n | Dificultad |
|----------|-------------|----------|-----------|
| **Multicolinealidad** | Alta varianza | Eliminar vars | Baja |
| **Heteroscedasticidad** | SE incorrectos | Errores robustos | Muy baja |
| **Autocorrelaci√≥n** | SE incorrectos | Errores HAC | Baja |
| **Endogeneidad** | Sesgo | IV, dise√±o causal | **Alta** |

---

class: inverse, center, middle

# Diagn√≥stico Visual

---

# ¬øPor qu√© gr√°ficos?

- Tests dicen **si** hay problema
- Gr√°ficos muestran **qu√© tipo** y **d√≥nde**
- Algunos problemas son obvios visualmente
- Ayudan a identificar outliers influyentes

---

# Los 4 gr√°ficos fundamentales

```r
par(mfrow = c(2, 2))
plot(modelo)
```

1. Residuals vs Fitted
2. Normal Q-Q
3. Scale-Location
4. Residuals vs Leverage

---

# 1. Residuals vs Fitted

**Qu√© muestra**: Residuos vs valores ajustados

**Qu√© buscar**:
- ‚úÖ **Bueno**: Nube sin patr√≥n alrededor de 0
- ‚ùå **Malo**:
  - Patr√≥n curvo ‚Üí No linealidad
  - Fan shape ‚Üí Heteroscedasticidad
  - Grupos separados ‚Üí Variable omitida

**El m√°s importante**

---

# 2. Normal Q-Q

**Qu√© muestra**: Cuantiles residuos vs normal te√≥rica

**Qu√© buscar**:
- ‚úÖ **Bueno**: Puntos siguen l√≠nea diagonal
- ‚ùå **Malo**:
  - Colas pesadas
  - Asimetr√≠a

**Importancia**:
- Normalidad para inferencia (tests)
- Con n &gt; 100, menos cr√≠tico

---

# 3. Scale-Location

**Qu√© muestra**: ‚àö|Residuos estandarizados| vs fitted

**Qu√© buscar**:
- ‚úÖ **Bueno**: L√≠nea horizontal
- ‚ùå **Malo**: L√≠nea con pendiente

**Ventaja**: M√°s f√°cil detectar hetero

---

# 4. Residuals vs Leverage

**Qu√© muestra**: Residuos vs leverage (influencia)

**Conceptos**:
- **Leverage**: Qu√© tan extrema es X
- **Distancia de Cook**: Influencia en coeficientes

**Qu√© buscar**:
- ‚úÖ **Bueno**: Puntos dentro de l√≠neas Cook
- ‚ùå **Malo**: Puntos fuera ‚Üí outliers influyentes

---

# Workflow de diagn√≥stico

**Paso 1**: Ajustar modelo

**Paso 2**: Plot 4 gr√°ficos
```r
par(mfrow = c(2,2))
plot(modelo)
```

**Paso 3**: Revisar problemas

**Paso 4**: Diagn√≥stico espec√≠fico
```r
bptest(modelo)  # Hetero
vif(modelo)     # Multicolin
```

**Paso 5**: Corregir

---

class: inverse, center, middle

# S√≠ntesis

---

# Puntos clave

1. **Regresi√≥n lineal**: Herramienta flexible para causal, predicci√≥n, descripci√≥n

2. **Supuestos Gauss-Markov**: Garantizan que MCO sea BLUE

3. **Interpretaci√≥n**: Depende de especificaci√≥n (nivel-nivel, log-log, etc.)

4. **Econometr√≠a vs ML**: Sesgo cr√≠tico vs performance predictiva

5. **Causalidad**: Requiere m√°s que regresi√≥n

6. **Problemas**: Endogeneidad m√°s grave; hetero f√°cil de corregir

7. **Diagn√≥stico visual**: Fundamental para detectar problemas

---

# Pr√≥xima clase

## Implementaci√≥n en R

- Carga y exploraci√≥n de wage1
- Especificaci√≥n de modelos
- Interpretaci√≥n con `broom`
- Diagn√≥stico con `car` y `lmtest`
- Correcci√≥n de heteroscedasticidad
- Variables instrumentales con `AER`
- Validaci√≥n out-of-sample
- Casos pr√°cticos completos

---

class: inverse, center, middle

# ¬øPreguntas?

---

# Referencias

**Textos fundamentales**:
1. Wooldridge, J. (2019). *Introductory Econometrics*, 7th ed.
2. James, G. et al. (2021). *An Introduction to Statistical Learning*, 2nd ed.
3. Angrist, J. &amp; Pischke, J.S. (2009). *Mostly Harmless Econometrics*

**Papers cl√°sicos**:
- Rubin, D. (1974). "Estimating Causal Effects of Treatments"
- White, H. (1980). "Heteroskedasticity-Consistent Covariance Matrix"
- Card, D. (1993). "Using Geographic Variation in College Proximity"

**Recursos online**:
- R for Data Science (r4ds.had.co.nz)
- Causal Inference: The Mixtape (mixtape.scunning.com)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
